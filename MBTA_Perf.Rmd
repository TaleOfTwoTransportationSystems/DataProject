---
title: "MBTA Performance Data"
author: "Shawn Connor, Jeff Cunningham, Danielle Feng and Varuni Gang"
date: "May 1, 2016"
output: html_document
---

## MBTA Performance Data

#### CSCI E-107 Final Project

#### Shawn Connor, Jeff Cunningham, Danille Feng and Varuni Gang

### Overview and Motivation
asdf

### Related work
asdf

### Initial Questions
asdf

### Data
asdf

Load Libraries:

```{r Load_Libraries, warning=FALSE, message=FALSE}
library(dplyr)
library(jsonlite)
library(lubridate)
library(readr)
library(tidyr)
library(leaflet)
library(ggplot2)
library(twitteR) #masks id, location from dplyr
library(stringr)
```

Some things we hard code:

```{r Hardcoded_Stuff}
# API information for the real-time feed
TKeyJeff <- "?api_key=tNprgYF1K027zEIVluAkCw"
#TKeyAPIDoc <- "?api_key=wX9NwuHnZU2ToO7GmGR9uw"
TRouteURL <- "http://realtime.mbta.com/developer/api/v2/stopsbyroute"
TTravelURL <- "http://realtime.mbta.com/developer/api/v2.1/traveltimes"
TFormat <- "&format=json"

startTime <- as.POSIXct("2016-01-25 04:00:00") # the start date of the class -- does affect how many archives are needed

TArchiveURLs <- c("http://www.mbta.com/uploadedfiles/MBTA_GTFS.zip",
                  "http://www.mbta.com/gtfs_archive/20151211.zip") # first is current archive; the rest are father back in time
```

asdf Google realtime performance blah blah

```{r Pull_Performance_Data}
RedLineRoute <- fromJSON(paste(TRouteURL, TKeyJeff, TFormat, "&route=Red", sep=""))[[1]]
MattapanLineRoute <- fromJSON(paste(TRouteURL, TKeyJeff, TFormat, "&route=Mattapan", sep=""))[[1]]

# Routes we're not using yet
#GreenBLineRoute <- fromJSON(paste(TRouteURL, TKeyJeff, TFormat, "&route=Green-B", sep=""))[[1]]
#GreenCLineRoute <- fromJSON(paste(TRouteURL, TKeyJeff, TFormat, "&route=Green-C", sep=""))[[1]]
#GreenDLineRoute <- fromJSON(paste(TRouteURL, TKeyJeff, TFormat, "&route=Green-D", sep=""))[[1]]
#GreenELineRoute <- fromJSON(paste(TRouteURL, TKeyJeff, TFormat, "&route=Green-E", sep=""))[[1]]
#BlueLineRoute <- fromJSON(paste(TRouteURL, TKeyJeff, TFormat, "&route=Blue", sep=""))[[1]]
#OrangeLineRoute <- fromJSON(paste(TRouteURL, TKeyJeff, TFormat, "&route=Orange", sep=""))[[1]]

# Build a list of all stop pairs of interest.
red_south <-  RedLineRoute$stop[[1]] %>%
  select(stop_order, stop_id) %>%
  arrange(stop_order) %>%
  mutate(next_stop = lead(stop_id)) %>%
  select(stop_id, next_stop) %>%
  filter(!is.na(next_stop))

red_north <-  RedLineRoute$stop[[2]] %>%
  select(stop_order, stop_id) %>%
  arrange(stop_order) %>%
  mutate(next_stop = lead(stop_id)) %>%
  select(stop_id, next_stop) %>%
  filter(!is.na(next_stop))

matt_out <-  MattapanLineRoute$stop[[1]] %>%
  select(stop_order, stop_id) %>%
  arrange(stop_order) %>%
  mutate(next_stop = lead(stop_id)) %>%
  select(stop_id, next_stop) %>%
  filter(!is.na(next_stop))

matt_in <-  MattapanLineRoute$stop[[2]] %>%
  select(stop_order, stop_id) %>%
  arrange(stop_order) %>%
  mutate(next_stop = lead(stop_id)) %>%
  select(stop_id, next_stop) %>%
  filter(!is.na(next_stop))

distinct_stop_pairs <- rbind(red_north, red_south, matt_out, matt_in)
rm(matt_in, matt_out, red_north, red_south)

distinct_stop_pairs_full <- distinct_stop_pairs


# The following can be skipped if "train_travel_times.csv" is present in the working directory
if(length(ls(pattern="travel_times")) > 0) {
    # do nothing - we already have the data in the environment
} else if (file.exists("train_travel_times.csv.gz")) {
    print("Loading previously generated data.")
    travel_times <- read_csv("train_travel_times.csv.gz") # can skip MBTA queries and load this instead
    names(travel_times)[1] <- "index" # adding a name to our unnamed first column
} else {
    print("Requesting data from realtime.mbta.com...")
    # create a holding frame for the data; we do this outside the loops so that it will persist.
    travel_times <- data.frame(direction=as.numeric(character()),
                        dep_dt=as.POSIXct(character()), 
                        arr_dt=as.POSIXct(character()), 
                        travel_time_sec=as.numeric(character()),
                        benchmark_travel_time_sec=as.numeric(character()),
                        from_stop=character(), 
                        to_stop=character()) 
    
    # How many seven day periods from start to now?
    numWeeks <- as.integer(unclass(now() - startTime)/7)
    
    # The outer loop cycles through every distinct pair of stops.
    for(j in 1:nrow(distinct_stop_pairs)) {
      from_j <- distinct_stop_pairs[j,]$stop_id
      to_j <- distinct_stop_pairs[j,]$next_stop
      fromStop <- paste("&from_stop=", from_j, sep="")
      toStop <- paste("&to_stop=", to_j, sep="")
      print(paste("Requesting", from_j, "to", to_j))
      
      # The inner loop cycles through each week of interest.  
      for(i in 0:numWeeks) {
        fromTime <- paste("&from_datetime=", as.numeric(startTime + days(i * 7)), sep="")
        toTime <- paste("&to_datetime=", as.numeric(startTime + days(i * 7) + days(7) - minutes(1)), sep="")
        TRequest <- paste(TTravelURL, TKeyJeff, TFormat, fromStop, toStop, fromTime, toTime, sep="")
        foo <- fromJSON(TRequest)[[1]]
        
        # Assuming we get a result back, we process it within the
        # inner loop, reformatting columns and dropping any we don't
        # plan to use. We then append it to travel_times.
        if (length(foo) > 0) {
          bar <- foo %>%
            mutate(from_stop = from_j,
              to_stop = to_j,
              dep_dt = as.POSIXct(as.integer(dep_dt), origin="1970-01-01"),
              arr_dt = as.POSIXct(as.integer(arr_dt), origin="1970-01-01"),
              travel_time_sec = as.numeric(travel_time_sec),
              benchmark_travel_time_sec = as.numeric(benchmark_travel_time_sec)) %>%
            select(-route_id, -contains("threshold"))
          travel_times <- rbind(travel_times, bar)
        } else {
          print(paste("Nothing returned for", fromStop, "to", toStop, "during period", fromTime, "-", toTime))
        }
        Sys.sleep(2) #slow down a bit
      }
    }
    
    # splitting date & time
    travel_times <- mutate(travel_times, dep_d=as.Date(dep_dt), 
                                                 dep_t=format(as.POSIXct(dep_dt), format="%H:%M:%S"), 
                                                 arr_d=as.Date(arr_dt), 
                                                 arr_t=format(as.POSIXct(arr_dt), format="%H:%M:%S"))
    # adding parent_station_name, lat and lon
    travel_times <- bind_rows(RedLineRoute$stop[1][[1]], RedLineRoute$stop[2][[1]]) %>% 
                        select(stop_id, parent_station_name, stop_lat, stop_lon) %>% 
                        mutate(stop_id=as.integer(stop_id)) %>% 
                        rename(to_stop = stop_id, to_name = parent_station_name, to_lat = stop_lat, to_lon = stop_lon) %>% 
                        inner_join(travel_times, by="to_stop")
    travel_times <- bind_rows(RedLineRoute$stop[1][[1]], RedLineRoute$stop[2][[1]]) %>% 
                        select(stop_id, parent_station_name, stop_lat, stop_lon) %>% 
                        mutate(stop_id=as.integer(stop_id)) %>% 
                        rename(from_stop = stop_id, from_name = parent_station_name, from_lat = stop_lat, from_lon = stop_lon) %>% 
                        inner_join(travel_times, by="from_stop")
    travel_times <- arrange(travel_times, direction, dep_dt)
    
    z <- gzfile("train_travel_times.csv.gz")
    write.csv(travel_times, z) #so others don't need to pull the data again
}
```

asdf This is the data we were going to orginally compare to Hubway, etc. but it's only _schedule_ data! blah blah

```{r Pull_Archive_Data, cache=TRUE, warning=FALSE}
for(i in 1:length(TArchiveURLs)) {
    if(file.exists(basename(TArchiveURLs[i]))) {
        print(paste("Using existing", basename(TArchiveURLs[i])))
    } else {
        download.file(TArchiveURLs[i], destfile=basename(TArchiveURLs[i]))
    }
    unzip(basename(TArchiveURLs[i]), exdir=strsplit(basename(TArchiveURLs[i]), "\\.")[[1]][1], overwrite=FALSE)
}

# The following is fixed for two archives; should be loop based on length of TArchivesURLs
setwd(strsplit(basename(TArchiveURLs[1]), "\\.")[[1]][1])

# Get schedule data:
Tarchive_cal <- read_csv("calendar.txt") %>% mutate(zip_file=basename(getwd()))
Tarchive_calDates <- read_csv("calendar_dates.txt")
Tarchive_trips <- read.csv("trips.txt")
Tarchive_stopTimes <- read.csv("stop_times.txt")
Tarchive_stops <- read.csv("stops.txt")

# OK, as a first step, let's trim back the Tarchive tables to just the subway (RTL) data:
Tarchive_cal <- filter(Tarchive_cal, grepl("RTL", service_id))
Tarchive_calDates <- filter(Tarchive_calDates, grepl("RTL", service_id)) # exception_type; 1=added; 2=removed
Tarchive_trips <- filter(Tarchive_trips, grepl("RTL", service_id))
Tarchive_stopTimes <- filter(Tarchive_stopTimes, trip_id %in% Tarchive_trips$trip_id)

setwd("..")

# Loop, please:
setwd(strsplit(basename(TArchiveURLs[2]), "\\.")[[1]][1])

Tarchive_cal2 <- read_csv("calendar.txt") %>% mutate(zip_file=basename(getwd()))
Tarchive_calDates2 <- read_csv("calendar_dates.txt")
Tarchive_trips2 <- read.csv("trips.txt")
Tarchive_stopTimes2 <- read.csv("stop_times.txt")

# trim2 -- dealing with multiple archive files really cries out for a function here, doesn't it
Tarchive_cal2 <- filter(Tarchive_cal2, grepl("RTL", service_id))
Tarchive_calDates2 <- filter(Tarchive_calDates2, grepl("RTL", service_id)) # exception_type; 1=added; 2=removed
Tarchive_trips2 <- filter(Tarchive_trips2, grepl("RTL", service_id))
Tarchive_stopTimes2 <- filter(Tarchive_stopTimes2, trip_id %in% Tarchive_trips$trip_id)

# combine our archives
Tarchive_cal <- bind_rows(Tarchive_cal, Tarchive_cal2)
Tarchive_calDates <- bind_rows(Tarchive_calDates, Tarchive_calDates2)
Tarchive_trips <- bind_rows(Tarchive_trips, Tarchive_trips2)
Tarchive_stopTimes <- bind_rows(Tarchive_stopTimes, Tarchive_stopTimes2)

setwd("..")

rm(Tarchive_cal2)
rm(Tarchive_calDates2)
rm(Tarchive_trips2)
rm(Tarchive_stopTimes2)

# Convert the dates:
Tarchive_cal <- mutate(Tarchive_cal, start_date=as.Date(as.character(start_date), format="%Y%m%d", origin="1970-01-01"), 
                                     end_date=as.Date(as.character(end_date), format="%Y%m%d", origin="1970-01-01"))
Tarchive_calDates <- mutate(Tarchive_calDates, date=as.Date(as.character(date), format="%Y%m%d", origin="1970-01-01"))

# Now, loop through each day between the start date and today,
# for each day, get the relevant service_ids and,
# find the corresponding trip_ids, then
# pull all the corresponding train arrival and departure times
schedule_dataset <- data.frame()
for(i in 0:(as.integer(unclass(now() - startTime)))) {
    iDate <- as.Date(startTime+days(i))
    
    #service_ids
    todaysServices <- filter(Tarchive_cal, start_date<=iDate & end_date>=iDate) %>%
        select(service_id) %>%
        distinct()

    #remove exceptions
    if(iDate %in% Tarchive_calDates$date) {
        servicesRemoved <- unique(filter(Tarchive_calDates, date==iDate & exception_type==2)[[1]])
        if(length(servicesRemoved)>1) {
            todaysServices <- filter(todaysServices, !service_id %in% servicesRemoved)
        }
    }
    
    #remove remaining regularly scheduled services that don't match this day of the week
    if(wday(iDate)==1) {
        todaysServices <- filter(todaysServices, grepl("Sunday", service_id))
    } else if(wday(iDate)==7) {
        todaysServices <- filter(todaysServices, grepl("Saturday", service_id))
    } else {
        todaysServices <- filter(todaysServices, grepl("Weekday", service_id))
    }
        
    #add exceptions
    if(iDate %in% Tarchive_calDates$date) {
        servicesAdded <- filter(Tarchive_calDates, date==iDate & exception_type==1) %>% 
                         select(service_id) %>% 
                         distinct()
        if(nrow(servicesAdded)>1) {
            todaysServices <- bind_rows(todaysServices, servicesAdded)
        }
    }
        
    #trip_ids
    todaysTrips <- filter(Tarchive_trips, service_id %in% todaysServices$service_id) %>% 
                  filter(route_id=="Red") %>% 
                  distinct()
#     print(paste(i, "; Date: ", iDate, "; # Services: ", nrow(todaysServices), "; # Trips: ", nrow(todayTrips), 
#                 "; Exception = ", (iDate %in% Tarchive_calDates$date), sep=""))

    #stop_times
    todaysStops <- filter(Tarchive_stopTimes, trip_id %in% todaysTrips$trip_id) %>% 
                   mutate(arrival_date=iDate, departure_date=iDate)
    
    schedule_dataset <- bind_rows(schedule_dataset, todaysStops)
}
```

asdf

```{r Headways}
#TBD -- Shawn's looking into this
```

asdf Alerts come in from Twitter via CodeForBoston blah blah

```{r Pull_Alerts}
# The following can be skipped if "red_line_tweets.csv" is present in the working directory
if (file.exists("red_line_tweets.csv.gz")) {
    print("Loading previously generated data.")
    red_line_alert<- read_csv("red_line_tweets.csv.gz") # can skip Twitter queries and load this instead
} else {
    # generated from twitter's website
    consumer_key <- 'bpCAcAY27kfpSyOAOFXNP2PsO'
    consumer_secret <- '5skjmU5FgWUA77PI4OwuBLcmv3Rr03xEKZQoG0FJJbI0wt3oMa'
    access_token <- '111824999-KVpkYnMt3MZU2Bfxl9lcHZfMvdF5pYZiHQqSonE6'
    access_secret <- 'Ib5N3qKxZ7CT1TuQeznHv6XobdCmjZkSVTESkVj7TwVZm'
    
    # authorizing twitter 
    setup_twitter_oauth(consumer_key = consumer_key, 
                        consumer_secret =  consumer_secret, 
                        access_token = access_token, 
                        access_secret = access_secret)
    
    # makes dataframe for results and gets specific features
    getSpecificTweetInformation <- function(x) {
        twListToDF(x) %>% 
            select(screenName, id, text, created, favoriteCount, retweetCount)
    }
    
    # Red Line Alerts user time line only 3200 hard limit
    Red_Line_Alerts_tweets <- userTimeline('Red_Line_Alerts', n=3200, includeRts = FALSE, excludeReplies = TRUE)
    
    # MattaPan Line Alerts user time line only 3200 hard limit
    highspeedalerts_tweets <- userTimeline('highspeedalerts', n=3200, includeRts = FALSE, excludeReplies = TRUE)
    
    # For other twitter alerts: 
    # # MBTA user time line only 3200 hard limit
    # mbta_tweets <- userTimeline('MBTA', n=3200, includeRts = FALSE, excludeReplies = TRUE)
    # 
    # # MBTA Alerts user time line only 3200 hard limit
    # mbta_alerts_tweets <- userTimeline('mbta_alerts', n=3200, includeRts = FALSE, excludeReplies = TRUE)
    # 
    # # Green Line Alerts user time line only 3200 hard limit
    # GreenLineAlerts_tweets <- userTimeline('GreenLineAlerts', n=3200, includeRts = FALSE, excludeReplies = TRUE)
    # 
    # # Orange Line Alerts user time line only 3200 hard limit
    # OrangeLineAlert_tweets <- userTimeline('OrangeLineAlert', n=3200, includeRts = FALSE, excludeReplies = TRUE)
    # 
    # # Blue Line Alerts user time line only 3200 hard limit
    # BlueLineAlerts_tweets <- userTimeline('BlueLineAlerts', n=3200, includeRts = FALSE, excludeReplies = TRUE)
    

    # Combining tweets from different twitter handles
    red_line_alert <- getSpecificTweetInformation(Red_Line_Alerts_tweets) %>% 
        union(getSpecificTweetInformation(highspeedalerts_tweets)) 
    
    # write all to a csv
    z <- gzfile("red_line_tweets.csv.gz")
    write.csv(mbta_tweets_combined, z) 
}

# tweet count and oldest and most recent tweet dates
red_line_alert %>% 
    group_by(screenName) %>% 
    summarise(num_of_tweets=n(), 
              oldest_date=min(as.Date.POSIXct(created)), 
              most_recent_date=max(as.Date.POSIXct(created)))
```

asdf

```{r Compare_Alerts_To_Performance}
# To relate to a data we already have is to add 
# these tweets as "arrival date" ~ "created" and then join them.
red_line_alert <- red_line_alert %>% 
    filter(created > startTime) %>%
    arrange(created) %>% 
    select(text, created, favoriteCount, retweetCount) %>% 
    mutate(arr_dt = created)

travel_times <- travel_times %>% 
    full_join(red_line_alert)

# To add tweets as a parameter for stops is by 
# pulling in stop names from the tweets and then mapping them to stop ids.

# First step is to pull in all the relevant stop ids from our full dataset
stop_ids <- c(travel_times$from_stop, travel_times$to_stop) %>% unique()

# Second step is get the actaul stop names for these stops
stop_names_codes <- Tarchive_stops %>% 
    filter(stop_id %in% stop_ids) %>% 
    select(stop_code, stop_name) %>% 
    unite(stop_code_name, stop_code, stop_name)

# Steps that I am taking to get the tweet-station pair
# 1. Pull in if the delay or issue is from a north_bound or south_bound or both trips
red_line_alert <- red_line_alert %>% 
    mutate(bounded = ifelse(grepl("northbound", text, ignore.case = TRUE), "northbound",
                            ifelse(grepl("southbound", text, ignore.case = TRUE), "southbound",
                                   ifelse(grepl("both ways|both direction", text, ignore.case = TRUE), "northbound_and_southbound", ""))))

# (side-step) level of severity ~ would be cool to draw a graph to see
# if severity increases with delay time or
# time series of severity increasing in the north/south bound trains
red_line_alert <- red_line_alert %>% 
    mutate(severity = ifelse(grepl("minor delay",text, ignore.case = TRUE), 1, 
                             ifelse(grepl("moderate delay",text, ignore.case = TRUE), 2, 
                                    ifelse(grepl("severe delay",text, ignore.case = TRUE), 3, 0))))

# Things are a bit tricky here, because red line has very different 
# different meaning of southbound/northbound ~ inbound/putbound trains
# From the information online 
# Red Line:  Toward Park Street (Green Line intersection) is Inbound; away is Outbound
# http://www.boston-discovery-guide.com/boston-subway.html
# 
# Alewife -- inbound/southbound---> Park street  <--- inbound/northbound---- Braintree/Ashmont
# Alewife <--outbound/northbound--- Park street  ---outbound/southbound----> Braintree/Ashmont
# Lets make a map of stops that are northbound and southbound

# stations with inbound/outbounds, northbound/southbound
northbound_inbound <- c("Braintree", 
                        "Quincy Adams",
                        "Quincy Center",
                        "Wollaston",
                        "North Quincy",
                        "JFK/UMASS Braintree",
                        "Broadway",
                        "South Station",
                        "Downtown Crossing - to Alewife",
                        "Savin",
                        "Fields",
                        "Shawmut",
                        "Ashmont",
                        "Park")

northbound_outbound <- c("Charles",
                         "Kendal",
                         "Central",
                         "Harvard",
                         "Porter",
                         "Davis",
                         "Alewife")

# southbound_outbound <- northbound_inbound # contains same stations
# southbound_inbound <- northbound_outbound # contains same stations

# One way of thinking about it in terms of getting to features are:
# A) creating common for recognizing train station where delay is been tweeted
red_line_alert <- red_line_alert %>% 
    mutate(text_clone = text) %>% 
    separate(text_clone, into = c("before_at", "after_at"), sep=" at ") %>% 
    mutate(after_at = str_trim(gsub("#mbta|Station|Ave|Street|[.]", "", after_at, ignore.case = TRUE))) %>% 
    rowwise() %>% 
    mutate( after_at = ifelse(!is.na(after_at),
                              ifelse(bounded != "", 
                                     ifelse(length(agrep(after_at, northbound_inbound, ignore.case = TRUE, value = TRUE,max =6))>0 & bounded == "northbound",
                                            paste(after_at, "inbound"),
                                            ifelse(length(agrep(after_at, northbound_inbound, ignore.case = TRUE,max =6))>0 & bounded == "southbound",
                                                   paste(after_at, "outbound"),
                                                   after_at
                                            )
                                     )
                                     , 
                                     ifelse(length(agrep(after_at, northbound_outbound, ignore.case = TRUE, value = TRUE,max =6))>0 & bounded == "northbound",
                                            paste(after_at, "outbound"),
                                            ifelse(length(agrep(after_at, northbound_outbound, ignore.case = TRUE, value = TRUE, max =6))>0 & bounded == "southbound",
                                                   paste(after_at, "inbound"),
                                                   after_at
                                            )
                                     ))
                              , after_at),
            after_at_name_code = 
                ifelse(!is.na(after_at),
                       toString(agrep(after_at, stop_names_codes$stop_code_name, value = TRUE, ignore.case = TRUE)), '')) %>%
    ungroup() %>%
    select(-after_at, -before_at) %>% 
    rename(alerts_at_station_code = after_at_name_code) 
```

asdf

```{r Visuals}

```

### Final Analysis
asdf

```{r Debug}
#remove this when we're done asdf
ls.str()
```

